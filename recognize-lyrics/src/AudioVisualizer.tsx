import React, { useEffect, useRef, useState } from "react";
import * as THREE from "three";

interface SpeechRecognitionEvent extends Event {
  results: SpeechRecognitionResultList;
  resultIndex: number;
}

const AudioVisualizer: React.FC = () => {
  const mountRef = useRef<HTMLDivElement>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const dataArrayRef = useRef<Uint8Array | null>(null);
  const meshRef = useRef<THREE.Mesh | null>(null);
  const [isAudioReady, setIsAudioReady] = useState(false);
  let renderer: THREE.WebGLRenderer | null = null;

  useEffect(() => {
    if (!mountRef.current) return;
    console.log("Initializing Three.js...");

    // Three.js ê¸°ë³¸ ì„¤ì •
    const scene = new THREE.Scene();
    const camera = new THREE.PerspectiveCamera(
      75,
      window.innerWidth / window.innerHeight,
      0.1,
      1000
    );
    camera.position.z = 5;

    // ê¸°ì¡´ rendererê°€ ìžˆìœ¼ë©´ ì œê±°í•˜ê³  ìƒˆë¡œ ìƒì„±
    if (renderer) {
      renderer.dispose(); // ì´ì „ renderer ë¦¬ì†ŒìŠ¤ í•´ì œ
      mountRef.current.removeChild(renderer.domElement); // ì´ì „ ìº”ë²„ìŠ¤ ì œê±°
    }

    renderer = new THREE.WebGLRenderer();
    renderer.setSize(window.innerWidth, window.innerHeight);
    mountRef.current.appendChild(renderer.domElement); // ìƒˆë¡œìš´ ìº”ë²„ìŠ¤ ì¶”ê°€

    // ê¸°ë³¸ ì¡°ëª… ì¶”ê°€
    scene.add(new THREE.AmbientLight(0xffffff, 0.5)); // ì£¼ë³€ê´‘ ì¶”ê°€
    const light = new THREE.PointLight(0xffffff, 2, 100);
    light.position.set(5, 5, 5);
    scene.add(light);

    const geometry = new THREE.SphereGeometry(1, 64, 64);
    const material = new THREE.MeshStandardMaterial({
      color: 0xffffff, // í°ìƒ‰
      transparent: true,
      opacity: 0.5,
      wireframe: false,
    });
    const mesh = new THREE.Mesh(geometry, material);
    scene.add(mesh);
    meshRef.current = mesh;

    // ë§ˆì´í¬ ì˜¤ë””ì˜¤ ì„¤ì •
    const setupAudio = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: true,
        });
        const audioContext = new AudioContext();
        const source = audioContext.createMediaStreamSource(stream);
        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 512;
        source.connect(analyser);

        analyserRef.current = analyser;
        dataArrayRef.current = new Uint8Array(analyser.frequencyBinCount);

        setIsAudioReady(true); // ì˜¤ë””ì˜¤ ì„¤ì • ì™„ë£Œ í›„ ìƒíƒœ ë³€ê²½

        console.log("ðŸ”Š ì˜¤ë””ì˜¤ ë¶„ì„ê¸° ì„¤ì • ì™„ë£Œ!");
      } catch (error) {
        console.error("ì˜¤ë””ì˜¤ ìž…ë ¥ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤:", error);
      }
    };

    setupAudio();

    // ì• ë‹ˆë©”ì´ì…˜ ë£¨í”„
    const animate = () => {
      requestAnimationFrame(animate);

      if (renderer) {
        renderer.render(scene, camera);
      }

      if (analyserRef.current && dataArrayRef.current && meshRef.current) {
        analyserRef.current.getByteFrequencyData(dataArrayRef.current);

        // ì†Œë¦¬ í¬ê¸° ë¶„ì„ â†’ êµ¬ì²´ í¬ê¸° ì¡°ì ˆ
        const volume =
          dataArrayRef.current.reduce((a, b) => a + b, 0) /
          dataArrayRef.current.length;
        const scale = 1 + volume / 50;
        meshRef.current.scale.set(scale, scale, scale);

        const bass =
          dataArrayRef.current.slice(0, 32).reduce((a, b) => a + b, 0) / 32;
        const treble =
          dataArrayRef.current.slice(100, 256).reduce((a, b) => a + b, 0) / 156;

        // ìƒ‰ìƒ ì—…ë°ì´íŠ¸ (bassì™€ treble ê°’ì— ë”°ë¥¸ ìƒ‰ìƒ ë³€í™”)
        const color = new THREE.Color(
          Math.min(bass / 256, 1),
          Math.min(1 - bass / 256, 1),
          Math.min(treble / 256, 1)
        );
        (meshRef.current.material as THREE.MeshStandardMaterial).color.set(
          color
        );

        // ë¬¼ë¦¬ì  íš¨ê³¼: ì§„ë™, ë³€í˜• ë“± ì¶”ê°€ ê°€ëŠ¥
        const time = performance.now() * 0.002;
        meshRef.current.rotation.x = time;
        meshRef.current.rotation.y = time;
      }
    };

    animate();

    // ì°½ í¬ê¸° ì¡°ì ˆ ëŒ€ì‘
    const onResize = () => {
      if (renderer && camera) {
        camera.aspect = window.innerWidth / window.innerHeight;
        camera.updateProjectionMatrix();
        renderer.setSize(window.innerWidth, window.innerHeight);
      }
    };
    window.addEventListener("resize", onResize);

    return () => {
      console.log("Cleaning up...");
      window.removeEventListener("resize", onResize);
      if (renderer) {
        renderer.dispose();
      }
      scene.clear();
      if (mountRef.current && renderer) {
        mountRef.current.removeChild(renderer.domElement);
      }
    };
  }, []);

  return (
    <div ref={mountRef} style={{ width: "100vw", height: "100vh" }}>
      {!isAudioReady && <p>Loading Audio...</p>}
    </div>
  );
};

export default AudioVisualizer;
