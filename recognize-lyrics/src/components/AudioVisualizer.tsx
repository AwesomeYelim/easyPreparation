import React, { useEffect, useRef, useState } from "react";
import * as THREE from "three";

interface SpeechRecognitionEvent extends Event {
  results: SpeechRecognitionResultList;
  resultIndex: number;
}

declare global {
  interface Window {
    SpeechRecognition: any;
    webkitSpeechRecognition: any;
  }
}

const songLyrics = [
  "ì˜ì§€í–ˆë˜ ëª¨ë“  ê²ƒ ë³€í•´ê°€ê³ \nì–µìš¸í•œ ë§ˆìŒì€ ì»¤ì ¸ê°€ë„¤",
  "ë¶€ë„ëŸ¼ ì—†ì´ ì‚´ê³  ì‹¶ì€ ë§˜\nì£¼ë‹˜ ì•„ì‹œë„¤",
  "ëª¨ë“  ì¼ì„ ì„ ìœ¼ë¡œ ì´ê²¨ë‚´ê³ \nì£„ì˜ ìœ í˜¹ì„ ë”°ë¥´ì§€ ì•Šë„¤",
  "ë‚˜ë¥¼ êµ¬ì›í•˜ì‹  ì˜ì›í•œ ê·¸ ì‚¬ëž‘\ní¬ì‹  ê·¸ ì€í˜œ ë‚  ë¶™ë“œì‹œë„¤",
  "ì£¼ì–´ì§„ ë‚´ ì‚¶ì´ ìž‘ê²Œë§Œ ë³´ì—¬ë„\nì„ í•˜ì‹  ì£¼ ë‚˜ë¥¼ ì´ë„ì‹¬ ë³´ë„¤",
  "ì¤‘ì‹¬ì„ ë³´ì‹œëŠ” ì£¼ë‹˜ë§Œ ë”°ë¥´ë„¤\në‚  íƒí•˜ì‹  ì£¼ë§Œ ì˜ì§€í•´",
  "ë³´ì´ëŠ” ìƒí™©ì— ë¬´ë„ˆì§ˆì§€ë¼ë„\nì˜ˆìˆ˜ ëŠ¥ë ¥ì´ ë‚˜ë¥¼ ë¶™ë“œë„¤",
  "ë³´ì´ì§€ ì•Šì•„ë„ ì£¼ë‹˜ë§Œ ë”°ë¥´ë„¤\në‚´ í‰ìƒ ì£¼ë‹˜ì„ ë…¸ëž˜í•˜ë¦¬ë¼",
  "ëª¨ë“  ì¼ì„ ì„ ìœ¼ë¡œ ì´ê²¨ë‚´ê³ \nì£„ì˜ ìœ í˜¹ì„ ë”°ë¥´ì§€ ì•Šë„¤",
  "ë‚˜ë¥¼ êµ¬ì›í•˜ì‹  ì˜ì›í•œ ê·¸ ì‚¬ëž‘\ní¬ì‹  ê·¸ ì€í˜œ ë‚  ë¶™ë“œì‹œë„¤",
  "ì£¼ì–´ì§„ ë‚´ ì‚¶ì´ ìž‘ê²Œë§Œ ë³´ì—¬ë„\nì„ í•˜ì‹  ì£¼ ë‚˜ë¥¼ ì´ë„ì‹¬ ë³´ë„¤",
  "ì¤‘ì‹¬ì„ ë³´ì‹œëŠ” ì£¼ë‹˜ë§Œ ë”°ë¥´ë„¤\në‚  íƒí•˜ì‹  ì£¼ë§Œ ì˜ì§€í•´",
  "ë³´ì´ëŠ” ìƒí™©ì— ë¬´ë„ˆì§ˆì§€ë¼ë„\nì˜ˆìˆ˜ ëŠ¥ë ¥ì´ ë‚˜ë¥¼ ë¶™ë“œë„¤",
  "ë³´ì´ì§€ ì•Šì•„ë„ ì£¼ë‹˜ë§Œ ë”°ë¥´ë„¤\në‚´ í‰ìƒ ì£¼ë‹˜ì„ ë…¸ëž˜í•˜ë¦¬ë¼",
];

const AudioVisualizer: React.FC = () => {
  const mountRef = useRef<HTMLDivElement>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const dataArrayRef = useRef<Uint8Array | null>(null);
  const meshRef = useRef<THREE.Mesh | null>(null);
  const [isAudioReady, setIsAudioReady] = useState(false);
  const [lyrics, setLyrics] = useState<string[]>([]);
  const [currentLine, setCurrentLine] = useState<number>(0);
  let renderer: THREE.WebGLRenderer | null = null;

  useEffect(() => {
    if (!mountRef.current) return;
    console.log("Initializing Three.js...");

    // Three.js ê¸°ë³¸ ì„¤ì •
    const scene = new THREE.Scene();
    const camera = new THREE.PerspectiveCamera(
      75,
      window.innerWidth / window.innerHeight,
      0.1,
      1000
    );
    camera.position.z = 5;

    // ê¸°ì¡´ rendererê°€ ìžˆìœ¼ë©´ ì œê±°í•˜ê³  ìƒˆë¡œ ìƒì„±
    if (renderer) {
      renderer.dispose(); // ì´ì „ renderer ë¦¬ì†ŒìŠ¤ í•´ì œ
      mountRef.current.removeChild(renderer.domElement); // ì´ì „ ìº”ë²„ìŠ¤ ì œê±°
    }

    renderer = new THREE.WebGLRenderer();
    renderer.setSize(window.innerWidth, window.innerHeight);
    mountRef.current.appendChild(renderer.domElement); // ìƒˆë¡œìš´ ìº”ë²„ìŠ¤ ì¶”ê°€

    // ê¸°ë³¸ ì¡°ëª… ì¶”ê°€
    scene.add(new THREE.AmbientLight(0xffffff, 0.5)); // ì£¼ë³€ê´‘ ì¶”ê°€
    const light = new THREE.PointLight(0xffffff, 2, 100);
    light.position.set(5, 5, 5);
    scene.add(light);

    const geometry = new THREE.SphereGeometry(1, 64, 64);
    const material = new THREE.MeshStandardMaterial({
      color: 0xffffff, // í°ìƒ‰
      transparent: true,
      opacity: 0.5,
      wireframe: false,
    });
    const mesh = new THREE.Mesh(geometry, material);
    scene.add(mesh);
    meshRef.current = mesh;

    // ë§ˆì´í¬ ì˜¤ë””ì˜¤ ì„¤ì •
    const setupAudio = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: true,
        });
        const audioContext = new AudioContext();
        const source = audioContext.createMediaStreamSource(stream);
        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 512;
        source.connect(analyser);

        analyserRef.current = analyser;
        dataArrayRef.current = new Uint8Array(analyser.frequencyBinCount);

        setIsAudioReady(true); // ì˜¤ë””ì˜¤ ì„¤ì • ì™„ë£Œ í›„ ìƒíƒœ ë³€ê²½

        console.log("ðŸ”Š ì˜¤ë””ì˜¤ ë¶„ì„ê¸° ì„¤ì • ì™„ë£Œ!");
      } catch (error) {
        console.error("ì˜¤ë””ì˜¤ ìž…ë ¥ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤:", error);
      }
    };

    setupAudio();

    // ì• ë‹ˆë©”ì´ì…˜ ë£¨í”„
    const animate = () => {
      requestAnimationFrame(animate);

      if (renderer) {
        renderer.render(scene, camera);
      }

      if (analyserRef.current && dataArrayRef.current && meshRef.current) {
        analyserRef.current.getByteFrequencyData(dataArrayRef.current);

        // ì†Œë¦¬ í¬ê¸° ë¶„ì„ â†’ êµ¬ì²´ í¬ê¸° ì¡°ì ˆ
        const volume =
          dataArrayRef.current.reduce((a, b) => a + b, 0) /
          dataArrayRef.current.length;
        const scale = 1 + volume / 100;
        meshRef.current.scale.set(scale, scale, scale);

        const bass =
          dataArrayRef.current.slice(0, 32).reduce((a, b) => a + b, 0) / 32;
        const treble =
          dataArrayRef.current.slice(100, 256).reduce((a, b) => a + b, 0) / 156;

        // ìƒ‰ìƒ ì—…ë°ì´íŠ¸ (bassì™€ treble ê°’ì— ë”°ë¥¸ ìƒ‰ìƒ ë³€í™”)
        const color = new THREE.Color(
          Math.min(bass / 256, 1),
          Math.min(1 - bass / 256, 1),
          Math.min(treble / 256, 1)
        );
        (meshRef.current.material as THREE.MeshStandardMaterial).color.set(
          color
        );

        // ë¬¼ë¦¬ì  íš¨ê³¼: ì§„ë™, ë³€í˜• ë“± ì¶”ê°€ ê°€ëŠ¥
        const time = performance.now() * 0.002;
        meshRef.current.rotation.x = time;
        meshRef.current.rotation.y = time;
      }
    };

    animate();

    // ì°½ í¬ê¸° ì¡°ì ˆ ëŒ€ì‘
    const onResize = () => {
      if (renderer && camera) {
        camera.aspect = window.innerWidth / window.innerHeight;
        camera.updateProjectionMatrix();
        renderer.setSize(window.innerWidth, window.innerHeight);
      }
    };
    window.addEventListener("resize", onResize);

    // ìŒì„± ì¸ì‹ ì„¤ì • (SpeechRecognition ë˜ëŠ” webkitSpeechRecognition ì‚¬ìš©)
    const recognition =
      window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!recognition) {
      console.error("SpeechRecognition API is not supported in this browser.");
      return;
    }

    const recognitionInstance = new recognition();
    recognitionInstance.lang = "ko-KR"; // í•œêµ­ì–´ë¡œ ì„¤ì •
    recognitionInstance.continuous = true;
    recognitionInstance.interimResults = true; // ì¤‘ê°„ ê²°ê³¼ í—ˆìš©

    recognitionInstance.onresult = (event: SpeechRecognitionEvent) => {
      const transcript = event.results[event.resultIndex][0].transcript;
      console.log("Recognized speech:", transcript);

      // transcriptì™€ ë§¤ì¹­ë˜ëŠ” ê°€ì‚¬ ì¤„ì„ ì°¾ê¸°
      const matchedLine = songLyrics.findIndex((line) =>
        line.includes(transcript)
      );

      if (matchedLine !== -1) {
        // í•´ë‹¹ ì¤„ì„ ì°¾ì•˜ìœ¼ë©´, ê·¸ ì¤„ë¡œ ì´ë™
        setLyrics([songLyrics[matchedLine]]);
        setCurrentLine(matchedLine); // ë§¤ì¹­ëœ ì¤„ë¡œ currentLine ì„¤ì •
      }
    };

    recognitionInstance.start(); // ìŒì„± ì¸ì‹ ì‹œìž‘

    return () => {
      console.log("Cleaning up...");
      window.removeEventListener("resize", onResize);
      if (renderer) {
        renderer.dispose();
      }
      scene.clear();
      if (mountRef.current && renderer) {
        mountRef.current.removeChild(renderer.domElement);
      }
    };
  }, [currentLine]);

  return (
    <div ref={mountRef} style={{ width: "100vw", height: "100vh" }}>
      {!isAudioReady && <p>Loading Audio...</p>}
      <div
        style={{
          position: "absolute",
          width: "70%",
          top: "50%",
          left: "50%",
          transform: "translate(-50%, -50%)",
          color: "white",
          fontSize: "5rem",
          fontWeight: "bold",
          fontFamily: "Nanum, sans-serif",
          textAlign: "center",
          lineHeight: "1.5",
        }}
      >
        {lyrics.map((line, index) => (
          <p key={index}>{line}</p>
        ))}
      </div>
    </div>
  );
};

export default AudioVisualizer;
